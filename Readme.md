# ü§ñ ANN Classification

A beginner-friendly **Artificial Neural Network (ANN)** model built using Python and TensorFlow/Keras to classify data with high accuracy. This project is part of my deep learning journey, showcasing how an ANN can be trained for binary and multi-class classification problems.

![TensorFlow](https://img.shields.io/badge/TensorFlow-FF6F00?style=for-the-badge&logo=tensorflow&logoColor=white)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)


---

## üöÄ Features

- ‚úÖ Build and train a custom ANN model  
- üìä Evaluate performance using metrics like accuracy, confusion matrix  
- üìà Visualize loss & accuracy curves  
- üî¨ Supports both binary and multi-class classification  
- üíæ Clean and modular codebase  

---

## üìÅ Project Structure
````
ANN-Classification/
‚îú‚îÄ‚îÄ dataset/ # (Optional) Dataset storage
‚îú‚îÄ‚îÄ models/ # Saved models
‚îú‚îÄ‚îÄ notebooks/ # Jupyter notebooks for experiments
‚îú‚îÄ‚îÄ outputs/ # Generated graphs & logs
‚îú‚îÄ‚îÄ utils/ # Helper functions
‚îú‚îÄ‚îÄ train.py # Script to train the model
‚îú‚îÄ‚îÄ evaluate.py # Script to evaluate the model
‚îî‚îÄ‚îÄ requirements.txt # Python dependencies
````


---

## üß† Model Architecture

The ANN is built with:

- Input Layer  
- Hidden Layers with ReLU activation  
- Output Layer (Softmax/Sigmoid based on task)  
- Optimizer: Adam  
- Loss: Binary/Categorical Crossentropy  

---

## üì∑ Sample Visualizations

> Make sure to generate and save your loss/accuracy plot in `outputs/` directory with filename `loss_accuracy_plot.png`.

<p align="center">
  <img src="outputs/loss_accuracy_plot.png" width="600" alt="Loss vs Accuracy">
</p>

---

## ‚öôÔ∏è Installation

```bash
git clone https://github.com/darshan-mishra17/ANN-Classification.git
cd ANN-Classification
pip install -r requirements.txt
```
‚ñ∂Ô∏è Usage
Training the model:
```
python train.py
```
Evaluating the model:
```
python evaluate.py
```
#üí° Future Improvements
Add support for image datasets (like MNIST/CIFAR)

Implement dropout/regularization

Add hyperparameter tuning options
